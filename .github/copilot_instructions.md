# Project: INSTITUTIONAL-GRADE XBRL FINANCIAL EXTRACTION PIPELINE

## Executive Summary
This project builds a production-grade, fully auditable financial data extraction pipeline that processes official BSE XBRL/XML filings (no PDFs/OCR), treating the XML instance as the immutable source of truth. It ensures determinism, lineage, explicit ambiguity tracking, and cumulative confidence scoring while handling Indian GAAP/Ind AS taxonomies, extensions, dimensional reporting, restatements, and structural inconsistencies. A shared RDF/OWL ontology and Neo4j graph model store all entities (companies, filings, facts, concepts, dimensions, validations) with full versioning metadata. Deterministic ingestion validates and normalizes XBRL structure, reconciles taxonomies using structural graph-based scoring, and consolidates facts with strict decimal and unit integrity while preserving raw values and lineage. LLMs are tightly contained to resolve semantic ambiguities in extension concepts and assist in probabilistic consistency checks, without altering deterministic data. Calculation validation, logical accounting checks, and anomaly detection generate quantified confidence impacts. Final outputs are strict schema-validated JSON objects containing extracted and normalized values, mapping confidence, validation flags, semantic scores, and complete traceability to the exact XML node, deployed via a Python-based.

## The 6-Phase Roadmap

- Intent: This project builds a production-grade, institutional financial extraction pipeline from official BSE XBRL/XML filings for hedge funds and investment managers. The XML instance is the immutable source of truth. No PDFs or OCR. The system must deliver determinism, auditability, lineage, ambiguity surfacing, and cumulative confidence scoring. It must handle Indian GAAP / Ind AS taxonomies, company-specific extensions, inconsistent presentation and calculation linkbases, scaling declarations, standalone vs consolidated contexts, dimensional reporting, restatements, and structural inconsistencies. All uncertainty must be explicit and traceable.

- Design: The system is modular by phase but shares a single canonical ontology and graph schema across all phases. The canonical model is implemented as an RDF/OWL financial ontology and operationalized in a Neo4j property graph. Core entities include Company, Filing, FinancialConcept, Fact, ReportingPeriod, ReportingLevel, Dimension, Unit, ExtensionConcept, and ValidationObservation, with relationships such as HAS_FACT, PRESENTED_UNDER, CALCULATES, EXTENDS, HAS_DIMENSION, DERIVED_FROM, and REPORTED_IN. All phases write to and read from this shared graph and canonical model. Every artifact is versioned by taxonomy version, filing version, processing timestamp, algorithm version, LLM model identifier, and prompt hash.

- Phase 1 performs deterministic XML ingestion and structural normalization using Arelle and lxml. The XBRL instance is validated against declared schemas, corrupted or incomplete filings terminate processing. All contexts, units, facts, linkbases, dimensions, footnotes, and extensions are parsed. Namespaces are normalized, QNames resolved, line numbers preserved, and each fact is stored with concept QName, raw value, contextRef, unitRef, decimals, precision, nil flag, and XML location. Contexts are canonicalized into deterministic period identifiers with explicit dimensional signatures. Units are resolved but not interpreted beyond declared definitions. Extensions are recorded with substitution group, data type, balance attribute, period type, abstract flag, and base references without semantic inference. Outputs are structured Parquet/JSON artifacts and graph nodes with full lineage.

- Phase 2 performs taxonomy reconciliation and canonical mapping. The official Indian taxonomy is loaded into the graph. Standard concepts map directly to canonical concepts. Extensions are aligned using deterministic structural scoring based on calculation tree similarity, presentation hierarchy proximity, balance type, period type, data type compatibility, dimensional usage clustering, and label similarity. Neo4j graph algorithms are used for node similarity and subgraph comparison. Multiple candidate mappings are preserved with confidence scores and structural evidence. No extension is discarded; unresolved concepts are flagged for semantic resolution.

- Phase 3 consolidates facts using Polars and Decimal arithmetic. Facts are grouped by canonical concept, reporting period, reporting level, and explicit dimension key. Standalone versus consolidated contexts are separated deterministically via context signals. Dimensional expansions are preserved, not flattened. Numeric normalization applies only declared decimals and unit definitions, raw and normalized values are stored separately. Duplicate or restated facts are resolved via deterministic authority hierarchy using filing version, context specificity, and restatement indicators. Each consolidated fact retains full lineage to original XML nodes and context identifiers.

- Phase 4 performs LLM-led semantic extension resolution under strict containment. The LLM receives structured JSON bundles containing unresolved extension concepts, structural neighbors, calculation and presentation positions, dimensional patterns, peer analogues, data type, and balance attributes. The LLM classifies extensions into canonical concepts, proposes equivalence or aggregation relationships, provides short structural justifications, and outputs explicit confidence scores in validated JSON schema. It may not hallucinate or alter facts. All semantic outputs are additive and versioned, never overwriting deterministic data.

- Phase 5 applies calculation validation and consistency analysis. Deterministic recalculation enforces calculation linkbase rules with tolerance derived strictly from decimals attributes. Structural mismatches are flagged without modification of reported data. LLM-assisted probabilistic reasoning evaluates logical accounting relationships such as Assets = Liabilities + Equity, cross-statement linkages, and anomalous dimensional distributions. Observations include quantified impact on cumulative confidence and structural root-cause hypotheses.

- Phase 6 produces the final institutional output model as strict schema-validated JSON. Each data point includes extracted_value, normalized_value, canonical_concept, original_concept_qname, reporting_period, reporting_level, dimensions, unit, scale, decimals, source_context_id, XML_line_reference, mapping_confidence, validation_flags, semantic_resolution_score, and cumulative_confidence_score. Ambiguity is preserved; multiple mappings may coexist with weighted confidence. Full lineage metadata includes taxonomy version, filing version, processing timestamp, algorithm versions, LLM model identifier, and reconciliation metadata.

- architecture: The architecture uses Python 3.12, Arelle for XBRL processing, lxml and xmlschema for validation, Polars for transformation, Decimal for numeric integrity, Pydantic for strict schema enforcement, Neo4j for graph storage and graph data science algorithms, RDFLib for ontology representation, Prefect for orchestration, Parquet for storage, Docker for containerization, and AWS production deployment via S3, ECS Fargate, Neo4j Aura, RDS, and Athena. Deterministic logic handles structure, normalization, and arithmetic; LLMs handle semantic ambiguity and probabilistic reasoning; no inferred scaling or arithmetic adjustments are allowed without explicit declaration; all confidence is cumulative, quantitative, and traceable; and every output must be auditable back to the exact XML node in the original filing.

- Instructions:
Let us start with a proof of concept. If the proof of concept works, we will move to production deployment. For now, let us focus on proof of concept in my dev environment. 
-- Dev environment: I am working on a windows 11 system with 16 gb. I am using VS Code. I have initiated git. I have created a remote repo on github. I have created a python virtual environment called venv. 

## Project State & Discovery
Whenever a session starts AND / OR whenever I ask you to do a "Project State & Discovery"
1. Scan all files in repository. Ignore those listed in .gitignore. Ignore "temp" folder. 
2. Identify which of the tasks in which of the 6 phases have been implemented already and which are yet to be implemented. 
3. Analyze the `imports` and `logic` in the existing modules to determine the current technical depth.
4. Compare the current files and code in these files and other artifacts against the "6-Phase Roadmap" and the related tasks.
5. Identify what should be our next task and sub-tasks which we can reasonalby complete in next 4 hours. 
6. Propose the specific tasks sub-tasks and help me complete each of those sub-tasks and tasks step by step. 